{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classifier\n",
    "\n",
    "### Naive Bayes Classfier to Predict incoming email as __Spam__ or __Ham__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn for feature extraction & modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Iteratively read files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For displaying images in ipython\n",
    "from IPython.display import HTML, display\n",
    "# Plotting libraries\n",
    "from IPython.display import SVG\n",
    "#from graphviz import Source\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes = True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data\n",
    "enron email files ref: http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined function to read and store bbc data from multipe folders\n",
    "def load_data(top_folders,folder_names,root_path):\n",
    "    doc_list = [] # initiate empty list to store text from files\n",
    "    \n",
    "    for name in top_folders:\n",
    "\n",
    "        fileNames = [path + '/' + name +'/'+ folder + '/*.txt' for path,folder in \n",
    "                     zip([root_path]*len(folder_names),folder_names )]\n",
    "\n",
    "        tags = folder_names\n",
    "        for docs in fileNames:\n",
    "            #print(docs)\n",
    "            #print(type(docs))\n",
    "            doc = glob.glob(docs) # glob method iterates through all the text documents in a folder\n",
    "            for text in doc:\n",
    "                with open(text, encoding='latin1') as f:\n",
    "                    tag = docs.split('/')[len(docs.split('/'))-2]\n",
    "                    lines = f.readlines()\n",
    "                    heading = lines[0].strip()\n",
    "                    body = ' '.join([l.strip() for l in lines[1:]])\n",
    "                    doc_list.append([tag, heading, body])\n",
    "            print(\"Completed loading data from folder: %s\"%tag)\n",
    "\n",
    "        print(\"Completed Loading text from %s folder\"%name)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paragpradhan/Documents/Data Science Course/DSB5/5. Text Mining'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron1 folder\n",
      "------------------------------------------------------------\n",
      "\n",
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron2 folder\n",
      "------------------------------------------------------------\n",
      "\n",
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron3 folder\n",
      "------------------------------------------------------------\n",
      "\n",
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron4 folder\n",
      "------------------------------------------------------------\n",
      "\n",
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron5 folder\n",
      "------------------------------------------------------------\n",
      "\n",
      "Completed loading data from folder: ham\n",
      "Completed loading data from folder: spam\n",
      "Completed Loading text from enron6 folder\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_folders = [\"enron1\",\"enron2\",\"enron3\",\"enron4\",\"enron5\",\"enron6\"]\n",
    "folder_names = ['ham','spam']\n",
    "docs = load_data(top_folders,folder_names = folder_names, root_path = os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Heading  \\\n",
      "0      ham                          Subject: ena sales on hpl   \n",
      "1      ham  Subject: 98 - 6736 & 98 - 9638 for 1997 ( ua 4...   \n",
      "2      ham    Subject: hpl nominations for december 28 , 1999   \n",
      "3      ham               Subject: revised nom - kcs resources   \n",
      "4      ham      Subject: new production - sitara deals needed   \n",
      "\n",
      "                                                Body  \n",
      "0  just to update you on this project ' s status ...  \n",
      "1  the above referenced meters need to be placed ...  \n",
      "2  ( see attached file : hpll 228 . xls ) - hpll ...  \n",
      "3  daren , it ' s in . bob - - - - - - - - - - - ...  \n",
      "4  daren , fyi . bob - - - - - - - - - - - - - - ...  \n",
      "\n",
      "Shape of data is (33716, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = pd.DataFrame(docs, columns=['Category', 'Heading', 'Body'])\n",
    "print(docs.head())\n",
    "print('\\nShape of data is {}\\n'.format(docs.shape))\n",
    "# print(docs.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_values = docs['Category'].value_counts()\n",
    "tags_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(8)\n",
    "fig.set_figheight(4)\n",
    "ax.bar(x = tags_values.index ,height = tags_values.values)\n",
    "#ax.barh(x = df['Gender'], height = np.mean(df.score))\n",
    "ax.set_title('Email Tag Counts')\n",
    "ax.set_xlabel('Email Tags')\n",
    "ax.set_ylabel('Counts')\n",
    "#ax.set_ylim(top = 0.2)\n",
    "for i, v in enumerate(tags_values.values):\n",
    "    ax.text(i-0.4, v + 0.01, s = np.round(v,2), color='black', fontweight='bold',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_ham = docs[docs[\"Category\"]==\"ham\"][\"Heading\"] \n",
    "print(heading_ham.shape)\n",
    "heading_ham[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_ham = docs[docs[\"Category\"]==\"ham\"][\"Heading\"] # Extract only heading of emails for Ham tags\n",
    "collapsed_heading_ham = heading_ham.str.cat(sep=' ')\n",
    "\n",
    "heading_spam = docs[docs[\"Category\"]==\"spam\"][\"Heading\"] # Extract only heading of emails for spam tags\n",
    "collapsed_heading_spam = heading_spam.str.cat(sep=' ') # Combining all the emails into one large text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"Subject\",\"re\",\"fw\",\"fwd\"])\n",
    "\n",
    "print(\"Word Cloud for Genuine emails\")\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",max_words=50).generate(collapsed_heading_ham)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:1\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWord Cloud for Spam emails\")\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",max_words=50).generate(collapsed_heading_spam)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:1\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Spam emails from Given set of emails\n",
    "\n",
    "__Naive Bayes Classifier__ It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "<img src=\"images/naive_bayes.png\" alt=\"nb\" style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(docs[\"Body\"], docs[\"Category\"],random_state = 42,\n",
    "                                                   test_size = 0.20)\n",
    "X_train.shape,X_test.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Pipeline for raw text transformation\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words= \"english\")),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Naive Bayes Classifier is {}\".format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test data\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Normalization\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['ham','spam'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "# With normalization\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes= ['ham','spam'], normalize=True,title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"email_classfier_V1.0.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = joblib.load(\"email_classfier_V1.0.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict([\"You have won %5000000000 lottery\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration\n",
    "\n",
    "Go through the blog on prediction of sentiments from text using Naive Bayes Classfier\n",
    "link: https://medium.com/@martinpella/naive-bayes-for-sentiment-analysis-49b37db18bf8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
